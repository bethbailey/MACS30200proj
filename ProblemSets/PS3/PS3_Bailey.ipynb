{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Image Classification\n",
    "## 1. Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Preprocess the data by converting the data to a 2D tensor with individual values between 0 and 1\n",
    "## and making it categorical. Will break into training and testing set with \"validation_split\" parameter\n",
    "## in model.\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 * 28))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Models\n",
    "### i. Initial test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(512, activation='relu'))\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fit initial test\n",
    "history = network.fit(train_images, train_labels, epochs=200, batch_size=512, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
    "print('test_acc: ', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I plotted the validation set accuracy/loss against the training accuracy/loss to see the comparison.\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "#loss_values = history_dict['loss']\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "#plt.plot(epochs, loss_values, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss')\n",
    "plt.title('Validation Set Loss by Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#acc_values = history_dict['acc']\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "#plt.plot(epochs, acc_values, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation accuracy')\n",
    "plt.title('Validation Set Accuracy by Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Identify the epoch where the model's performance degrades based on the validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Implement dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_network = models.Sequential()\n",
    "dropout_network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
    "dropout_network.add(layers.Dropout(0.5))\n",
    "dropout_network.add(layers.Dense(512, activation='relu'))\n",
    "dropout_network.add(layers.Dropout(0.5))\n",
    "dropout_network.add(layers.Dense(512, activation='relu'))\n",
    "dropout_network.add(layers.Dropout(0.5))\n",
    "dropout_network.add(layers.Dense(512, activation='relu'))\n",
    "dropout_network.add(layers.Dropout(0.5))\n",
    "dropout_network.add(layers.Dense(10, activation='softmax'))\n",
    "dropout_network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_history = dropout_network.fit(train_images, train_labels, epochs=200, batch_size=512, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_test_loss, dropout_test_acc = dropout_network.evaluate(test_images, test_labels)\n",
    "print('dropout_test_acc: ', dropout_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_history_dict = dropout_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_val_loss_values = dropout_history_dict['val_loss']\n",
    "\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Initial Model Validation loss')\n",
    "plt.plot(epochs, dropout_val_loss_values, 'bo', label='Dropout Validation loss')\n",
    "plt.title('Validation Set Loss by Epoch (with and without Dropout)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How does this new model perform relative to the old model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Weight regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers \n",
    "\n",
    "# Do l1 regularization.\n",
    "l1_reg_network = models.Sequential()\n",
    "l1_reg_network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), \n",
    "                kernal_regularizer=regularizers.l1(0.001)))\n",
    "l1_reg_network.add(layers.Dense(512, activation='relu', kernal_regularizer=regularizers.l1(0.001)))\n",
    "l1_reg_network.add(layers.Dense(512, activation='relu', kernal_regularizer=regularizers.l1(0.001)))\n",
    "l1_reg_network.add(layers.Dense(512, activation='relu', kernal_regularizer=regularizers.l1(0.001)))\n",
    "l1_reg_network.add(layers.Dense(10, activation='softmax'))\n",
    "l1_reg_network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_reg_history = l1_reg_network.fit(train_images, train_labels, epochs=200, batch_size=512, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l1_reg_test_loss, l1_reg_test_acc = l1_reg_network.evaluate(test_images, test_labels)\n",
    "print('l1_reg_test_acc: ', l1_reg_test_acc)\n",
    "l1_reg_history_dict = l1_reg_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do l2 regularization.\n",
    "l2_reg_network = models.Sequential()\n",
    "l2_reg_network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,), \n",
    "                kernal_regularizer=regularizers.l2(0.001)))\n",
    "l2_reg_network.add(layers.Dense(512, activation='relu', kernal_regularizer=regularizers.l2(0.001)))\n",
    "l2_reg_network.add(layers.Dense(512, activation='relu', kernal_regularizer=regularizers.l2(0.001)))\n",
    "l2_reg_network.add(layers.Dense(512, activation='relu', kernal_regularizer=regularizers.l2(0.001)))\n",
    "l2_reg_network.add(layers.Dense(10, activation='softmax'))\n",
    "l2_reg_network.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_reg_history = l2_reg_network.fit(train_images, train_labels, epochs=200, batch_size=512, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l2_reg_test_loss, l2_reg_test_acc = l2_reg_network.evaluate(test_images, test_labels)\n",
    "print('l2_reg_test_acc: ', l2_reg_test_acc)\n",
    "l2_reg_history_dict = l2_reg_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot all the validation losses.\n",
    "l1_reg_val_loss_values = l1_reg_history_dict['val_loss']\n",
    "l2_reg_val_loss_values = l2_reg_history_dict['val_loss']\n",
    "\n",
    "plt.plot(epochs, val_loss_values, label='Initial Model Validation loss')\n",
    "plt.plot(epochs, dropout_val_loss_values, label='Dropout Validation loss')\n",
    "plt.plot(epochs, l1_reg_val_loss_values, label='L1 Validation loss')\n",
    "plt.plot(epochs, l2_reg_val_loss_values, label='L2 Validation loss')\n",
    "plt.title('Validation Set Loss by Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which model appears to perform the best?\n",
    "initial_min = np.min(val_loss_values)\n",
    "dropout_min = np.min(dropout_val_loss_values)\n",
    "l1_reg_min = np.min(l1_reg_val_loss_values)\n",
    "l2_reg_min = np.min(l2_reg_val_loss_values)\n",
    "print(\"Initial model min:\", initial_min)\n",
    "print(\"Dropout model min:\", dropout_min)\n",
    "print(\"L1 Reg model min:\", l1_reg_min)\n",
    "print(\"L2 Reg model min:\", l2_reg_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that performs the best is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iv. Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the lowest epoch:\n",
    "number_epochs =  np.argmin(l2_reg_val_loss_values) + 1\n",
    "print(\"number_epochs = \", number_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do l2 regularization with epochs and entire dataset.\n",
    "all_data_l2_reg_history = l2_reg_network.fit(train_images, train_labels, epochs=number_epochs,\n",
    "                                             batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data_l2_reg_test_loss, all_data_l2_reg_test_acc = all_data_l2_reg_network.evaluate(test_images, test_labels)\n",
    "print('Test set loss with L2 regularization with all data is ', all_data_l2_reg_test_loss)\n",
    "print('Test set accuracy with L2 regularization with all data is ', all_data_l2_reg_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# How well does your model perform to the baseline from chapter 2.1 in the book?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline model in the book has an accuracy of 0.9785. Thus, with accuracy of ----, this model performs slightly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Scalar Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.dataset import boston_housing\n",
    "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalizing the data.\n",
    "mean = train_data.mean(axis=0)\n",
    "train_data -= mean\n",
    "std = train_data.std(axis=0)\n",
    "train_data /= std \n",
    "\n",
    "test_data -= mean\n",
    "test_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build the simple model from the book.\n",
    "def build_simple_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add k-fold validation.\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "k = 10\n",
    "num_val_samples = len(train_data) // k\n",
    "num_epochs = 100\n",
    "all_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-2-ca05b3217256>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ca05b3217256>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for i in range(k):\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "def ten_fold_validate(model, k, num_epochs, all_scores):\n",
    "    \n",
    "    kf = KFold(n_splits = k)\n",
    "    fold = 0\n",
    "    for train_index, test_index in kf.split(train_data):\n",
    "        fold +=1 \n",
    "        data_train, data_test = train_data[train_index], train_data[test_index]\n",
    "        target_train, target_test = train_targets[train_index], train_targets[test_index]\n",
    "        history = model.fit(data_train, target_train, epochs=num_epochs, batch_size=1)\n",
    "        val_mse, val_mae = model.evaluate(val_data, val_targets)\n",
    "        all_scores.append(val_mse)\n",
    "        \n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = build_simple_model()\n",
    "simple_model_scores = ten_fold_validate(model, k, num_epochs, all_scores)\n",
    "print(\"Mean of all scores for simple model is:\", np.mean(simple_model_scores))\n",
    "print(\"Min of all scores for simple model is:\", np.min(simple_model_scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
